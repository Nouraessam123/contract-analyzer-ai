import streamlit as st
import os
import docx  # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„ÙˆÙˆØ±Ø¯
from langchain_community.document_loaders import PyPDFLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_groq import ChatGroq
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document # Ù„ØªØ­ÙˆÙŠÙ„ Ù†Øµ Ø§Ù„ÙˆÙˆØ±Ø¯ Ù„Ø´ÙƒÙ„ ÙŠÙÙ‡Ù…Ù‡ Ø§Ù„Ø³ÙŠØ³ØªÙ…
from dotenv import load_dotenv
# 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ù€ CSS
st.set_page_config(page_title="AI Legal Auditor", layout="wide")

st.markdown("""
    <style>
    .stApp { background-color: #f8f9fa; }
    h1, h2, h3 { color: #1e3d59; font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
    .stButton>button {
        width: 100%;
        border-radius: 8px;
        background-color: #1e3d59;
        color: white;
        font-weight: bold;
        border: none;
        height: 3em;
    }
    .stButton>button:hover { background-color: #ffc107; color: #1e3d59; border: 1px solid #1e3d59; }
    </style>
    """, unsafe_allow_html=True)

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„ÙØ§Øª Ø§Ù„ÙˆÙˆØ±Ø¯
def read_docx(file):
    doc = docx.Document(file)
    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Document Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ LangChain
    return [Document(page_content=text, metadata={"source": "uploaded_docx"})]

st.title("âš–ï¸ Ù…Ù†ØµØ© ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¹Ù‚ÙˆØ¯ Ø§Ù„Ø°ÙƒÙŠØ©")
st.write("Ù‚Ù… Ø¨Ø±ÙØ¹ Ø§Ù„Ø¹Ù‚Ø¯ (PDF Ø£Ùˆ Word) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø¨Ù…ÙŠØ²Ø§Ù† Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ.")
load_dotenv()
# 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
@st.cache_resource
def load_models():
    embeddings = HuggingFaceEmbeddings(model_name="./my_model")
    llm = ChatGroq(
        temperature=0, 
        # Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ù‡Ù†Ø§: Ø§Ø³Ù… Ø§Ù„Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ± Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† api_key
        api_key=os.getenv("GROQ_API_KEY"), 
        model_name="llama-3.3-70b-versatile"
    )
    return embeddings, llm

embeddings, llm = load_models()

# 3. Ø±ÙØ¹ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù (ÙŠØ¯Ø¹Ù… PDF Ùˆ DOCX)
uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ Ø§Ù„Ø¹Ù‚Ø¯", type=["pdf", "docx"])

if uploaded_file:
    file_extension = uploaded_file.name.split('.')[-1].lower()
    
    with st.spinner(f"Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ù {file_extension.upper()}..."):
        # Ø­ÙØ¸ Ù…Ù„Ù Ù…Ø¤Ù‚Øª Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡
        with open(f"temp.{file_extension}", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        # Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¨ÙŠÙ† Ø§Ù„Ù‚Ø§Ø±Ø¦ Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if file_extension == "pdf":
            loader = PyPDFLoader("temp.pdf")
            docs = loader.load()
        else:
            docs = read_docx(f"temp.docx")
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
        splits = text_splitter.split_documents(docs)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ Vector Store
        vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

    # 4. Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠØ©
    def run_legal_task(task_instruction, use_table=False):
        relevant_docs = retriever.invoke(task_instruction)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])
        
        table_instruction = "Ø¨Ø±Ø¬Ø§Ø¡ Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø´ÙƒÙ„ Ø¬Ø¯ÙˆÙ„ Markdown Ù…Ù†Ø¸Ù… (Ø§Ù„Ø¨Ù†Ø¯ | Ø§Ù„ØªÙØ§ØµÙŠÙ„)." if use_table else ""

        full_prompt = f"""Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù…ØµØ±ÙŠ Ø®Ø¨ÙŠØ±. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙ‚Ø· Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù‚Ø¯ ÙˆÙÙ‚Ø§Ù‹ Ù„Ø£ØµÙˆÙ„ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ.
        
        Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©:
        {context}
        
        Ø§Ù„Ù…Ù‡Ù…Ø©: {task_instruction}
        {table_instruction}
        
        Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        1. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙÙ‚Ø·.
        2. Ø§Ø¯Ø®Ù„ ÙÙŠ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª.
        3. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª ÙˆØ§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©."""
        
        with st.spinner("ÙŠØªÙ… Ø§Ù„Ø¢Ù† Ø§Ù„ÙØ­Øµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ..."):
            response = llm.invoke(full_prompt)
            return response.content

    # 5. Ø¹Ø±Ø¶ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª
    st.write("---")
    st.subheader("Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø³Ø±ÙŠØ¹Ø©:")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("ğŸ“ Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù‚Ø¯"):
            res = run_legal_task("Ù„Ø®Øµ Ø£Ù‡Ù… 5 Ø¨Ù†ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¹Ù‚Ø¯ (Ø§Ù„Ø£Ø·Ø±Ø§ÙØŒ Ø§Ù„Ù…Ø¯Ø©ØŒ Ø§Ù„Ù‚ÙŠÙ…Ø©ØŒ Ø§Ù„ØºØ±Ø¶ØŒ Ø§Ù„ÙØ³Ø®).", use_table=True)
            st.markdown(res)

    with col2:
        if st.button("ğŸš¨ ÙƒØ´Ù Ø§Ù„Ù…Ø®Ø§Ø·Ø±"):
            res = run_legal_task("Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø«ØºØ±Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ù…ØµØ±ÙŠ ÙÙŠ Ø´ÙƒÙ„ Ù†Ù‚Ø§Ø· ÙˆØ§Ø¶Ø­Ø©.")
            st.warning(res)

    with col3:
        if st.button("ğŸ’° Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª"):
            res = run_legal_task("Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„Ù…Ø°ÙƒÙˆØ±ØŸ", use_table=True)
            st.success(res)

    # 6. Ù‚Ø³Ù… Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
    st.divider()
    st.subheader("ğŸ’¬ Ø§Ø³Ø£Ù„ Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ")
    user_query = st.text_input("Ù…Ø«Ù„Ø§Ù‹: Ù…Ø§ Ù‡ÙŠ Ø´Ø±ÙˆØ· ÙØ³Ø® Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù‚Ø¯ØŸ")
    if user_query:
        answer = run_legal_task(user_query)
        st.write("**Ø§Ù„Ø±Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ:**")
        st.write(answer)

else:
    st.info("ğŸ’¡ **Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¨Ø¯Ø¡:** Ø§Ø±ÙØ¹ Ø¹Ù‚Ø¯Ùƒ Ø§Ù„Ø¢Ù† Ø¨ØµÙŠØºØ© **PDF** Ø£Ùˆ **Word**ØŒ ÙˆØ³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø°ÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø«ØºØ±Ø§Øª ÙˆØ§Ù„Ø§Ù„ØªØ²Ø§Ù…Ø§Øª ÙÙŠ Ø«ÙˆØ§Ù†Ù.")